server:
  api_key: 
  logging:
    enabled: true
    level: debug
    log_file: app.log
  proxy:
    enabled: false
    address: socks5://username:password@proxy.example.com:1080
  dashboard:
    enabled: true
  cors:
    # Allow all origins with "*", but specify exact origins when allow_credentials is true for security
    allow_origins: ["*"]
    allow_credentials: true
    allow_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    allow_headers: ["*"]

# Default configuration applied to all API endpoints unless overridden
default_settings:
  key_variable: keys
  key_concurrency: true # mark it as true if each key can handle multiple concurrent requests, otherwise the key will be locked until the request completes
  randomness: 0.0 # Random delay of (0.0-x)s to introduce variability in request timing and avoid detection due to consistent request patterns due to rate limits
  load_balancing_strategy: round_robin
  # Path and method filtering
  allowed_paths:
    enabled: false # Set to true to enable request path filtering
    mode: whitelist # if "whitelist", only allow listed paths; if "blacklist", block listed paths
    paths:
      - "*"
  allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"] # Allowed HTTP methods for the API
  queue:
    max_size: 200
    expiry_seconds: 300
  rate_limit:
    enabled: true
    endpoint_rate_limit: 10/s # Default endpoint rate limit - can be overridden per API
    key_rate_limit: 10/m # Default key rate limit - can be overridden per API
    ip_rate_limit: 1000/d # IP-based rate limit to protect against abuse and key redistribution
    user_rate_limit: 1000/d # User-based rate limit per proxy API key defined in server section
    rate_limit_paths: 
      - "*"
  retry:
    enabled: true
    mode: key_rotation
    attempts: 3
    retry_after_seconds: 1
    retry_request_methods: [ POST, GET, PUT, DELETE, PATCH, OPTIONS ]
    retry_status_codes: [ 429, 500, 502, 503, 504 ]
  timeouts:
    request_timeout_seconds: 300

apis:
  openai:
    # Example for OpenAI-compatible API endpoint
    name: Google Gemini API
    # Supported endpoints:
    # Gemini: https://generativelanguage.googleapis.com/v1beta/openai
    # OpenAI: https://api.openai.com/v1
    # Anthropic: https://api.anthropic.com/v1
    # DeepSeek: https://api.deepseek.com/v1
    # Mistral: https://api.mistral.ai/v1
    # OpenRouter: https://api.openrouter.ai/v1
    # Ollama: http://localhost:11434/v1
    endpoint: https://api.openai.com/v1
    aliases:
    - /openai
    key_variable: keys
    headers:
      Authorization: 'Bearer ${{keys}}'
    variables:
      keys:
      - your_openai_key_1
      - your_openai_key_2
      - your_openai_key_3
    load_balancing_strategy: least_requests
    rate_limit:
      enabled: true
      endpoint_rate_limit: 5000/d # Update this OpenAI endpoint rate limit as needed
      key_rate_limit: 100/m # Update this OpenAI key rate limit as needed
      rate_limit_paths:
        - "/chat/*"
        - "/images/*"